"""
Analisador de padrÃµes de temperatura usando Google Gemini AI
Requer: pip install google-generativeai python-dotenv
"""

import os
from datetime import datetime
from pathlib import Path

# Carregar variÃ¡veis do arquivo .env
try:
    from dotenv import load_dotenv
    # Carregar .env do diretÃ³rio do script
    env_path = Path(__file__).parent / '.env'
    load_dotenv(dotenv_path=env_path)
    print(f"[AI ANALYZER] Carregando configuraÃ§Ãµes de: {env_path}")
except ImportError:
    print("[AI ANALYZER] python-dotenv nÃ£o instalado - usando variÃ¡veis de ambiente do sistema")

# Verificar disponibilidade de APIs de IA
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    genai = None

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    Groq = None

if not GEMINI_AVAILABLE and not GROQ_AVAILABLE:
    print("[AI ANALYZER] âš ï¸ Nenhuma API de IA disponÃ­vel - rodando em modo automÃ¡tico")
    print("[AI ANALYZER] Instale: pip install google-generativeai groq")

class TemperatureAIAnalyzer:
    """Analisa padrÃµes de temperatura usando IA"""
    
    def __init__(self, api_key=None, groq_api_key=None, provider=None):
        """
        Args:
            api_key: Google Gemini API key (ou usa variÃ¡vel GEMINI_API_KEY)
            groq_api_key: Groq API key (ou usa variÃ¡vel GROQ_API_KEY)
            provider: 'gemini', 'groq' ou None (auto-detecta)
        """
        self.gemini_key = api_key or os.getenv('GEMINI_API_KEY')
        self.groq_key = groq_api_key or os.getenv('GROQ_API_KEY')
        self.provider = provider
        self.model = None
        self.groq_client = None
        
        # Auto-detectar qual provider usar
        if not self.provider:
            if self.groq_key and GROQ_AVAILABLE:
                self.provider = 'groq'
            elif self.gemini_key and GEMINI_AVAILABLE:
                self.provider = 'gemini'
        
        # Inicializar Groq
        if self.provider == 'groq' and GROQ_AVAILABLE and self.groq_key:
            try:
                self.groq_client = Groq(api_key=self.groq_key)
                print("[AI ANALYZER] ğŸš€ Groq AI configurado âœ… (RÃ¡pido & Gratuito)")
            except Exception as e:
                print(f"[AI ANALYZER] Erro ao configurar Groq: {e}")
                self.groq_client = None
        
        # Inicializar Gemini
        elif self.provider == 'gemini' and GEMINI_AVAILABLE and self.gemini_key:
            try:
                genai.configure(api_key=self.gemini_key)
                self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
                print("[AI ANALYZER] Google Gemini configurado âœ…")
            except Exception as e:
                print(f"[AI ANALYZER] Erro ao configurar Gemini: {e}")
                self.model = None
        
        if not self.model and not self.groq_client:
            print("[AI ANALYZER] Rodando sem IA (forneÃ§a GEMINI_API_KEY ou GROQ_API_KEY)")
    
    def analyze_temperature_data(self, readings, statistics=None):
        """
        Analisa padrÃµes de temperatura e gera insights
        
        Args:
            readings: Lista de leituras [{timestamp, temperature, anomaly}, ...]
            statistics: Dict com estatÃ­sticas opcionais
            
        Returns:
            Dict com anÃ¡lise ou fallback sem IA
        """
        if not self.model and not self.groq_client:
            print("[AI ANALYZER] Modelo nÃ£o configurado - usando fallback")
            return self._fallback_analysis(readings, statistics)
        
        try:
            # Preparar dados para IA
            prompt = self._build_prompt(readings, statistics)
            
            print(f"[AI ANALYZER] Chamando {self.provider.upper()} com {len(readings)} leituras...")
            
            # Chamar API com retry
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # Groq API
                    if self.provider == 'groq' and self.groq_client:
                        response = self.groq_client.chat.completions.create(
                            model="llama-3.3-70b-versatile",
                            messages=[{"role": "user", "content": prompt}],
                            temperature=0.7,
                            max_tokens=1024
                        )
                        response_text = response.choices[0].message.content
                    
                    # Gemini API
                    elif self.provider == 'gemini' and self.model:
                        response = self.model.generate_content(prompt)
                        if not response or not response.text:
                            raise ValueError("Resposta vazia do Gemini")
                        response_text = response.text
                    
                    else:
                        raise ValueError("Nenhum provider configurado")
                    
                    print(f"[AI ANALYZER] âœ… {self.provider.upper()} respondeu com sucesso!")
                    
                    return {
                        'ai_powered': True,
                        'provider': self.provider,
                        'analysis': response_text,
                        'timestamp': datetime.now().isoformat(),
                        'data_points': len(readings)
                    }
                    
                except Exception as retry_error:
                    print(f"[AI ANALYZER] Tentativa {attempt + 1}/{max_retries} falhou: {type(retry_error).__name__}: {retry_error}")
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(1)  # Aguardar antes de retry
                    else:
                        raise  # Re-raise na Ãºltima tentativa
            
        except Exception as e:
            print(f"[AI ANALYZER] âŒ ERRO CRÃTICO {self.provider.upper() if self.provider else 'IA'}: {type(e).__name__}")
            print(f"[AI ANALYZER] Detalhes: {str(e)}")
            print(f"[AI ANALYZER] Voltando para modo automÃ¡tico...")
            return self._fallback_analysis(readings, statistics)
    
    def _build_prompt(self, readings, statistics):
        """ConstrÃ³i prompt para Gemini"""
        
        # Resumo dos dados
        temps = [r['temperature'] for r in readings]
        anomalies = [r for r in readings if r.get('anomaly')]
        
        prompt = f"""
VocÃª Ã© um especialista em anÃ¡lise de processos industriais. Analise os seguintes dados de temperatura de um sistema industrial:

**Dados Coletados:**
- Total de leituras: {len(readings)}
- PerÃ­odo: {readings[0]['timestamp']} atÃ© {readings[-1]['timestamp']}
- Temperatura mÃ­nima: {min(temps):.2f}Â°C
- Temperatura mÃ¡xima: {max(temps):.2f}Â°C
- Temperatura mÃ©dia: {sum(temps)/len(temps):.2f}Â°C
"""
        
        if statistics:
            prompt += f"""
**EstatÃ­sticas:**
- Desvio padrÃ£o: {statistics.get('stdev', 0):.2f}Â°C
- Anomalias detectadas: {statistics.get('anomalies', 0)}
"""
        
        # Amostra de valores recentes
        recent = readings[-20:]
        prompt += "\n**Ãšltimas 20 leituras:**\n"
        for r in recent:
            marker = "âš ï¸" if r.get('anomaly') else "  "
            prompt += f"{marker} {r['timestamp']}: {r['temperature']:.2f}Â°C\n"
        
        prompt += """

**Tarefa:**
Analise estes dados e forneÃ§a:

1. **TendÃªncia**: A temperatura estÃ¡ estÃ¡vel, crescente ou decrescente?
2. **PadrÃµes**: HÃ¡ ciclos ou variaÃ§Ãµes periÃ³dicas?
3. **Anomalias**: As variaÃ§Ãµes bruscas indicam problema?
4. **RecomendaÃ§Ãµes**: SugestÃµes para otimizaÃ§Ã£o ou alertas

Seja conciso e objetivo. Foque em insights prÃ¡ticos para o operador.
"""
        
        return prompt
    
    def _fallback_analysis(self, readings, statistics):
        """AnÃ¡lise bÃ¡sica sem IA"""
        
        if not readings:
            return {
                'ai_powered': False,
                'analysis': 'Sem dados suficientes para anÃ¡lise',
                'timestamp': datetime.now().isoformat()
            }
        
        temps = [r['temperature'] for r in readings]
        anomalies = sum(1 for r in readings if r.get('anomaly'))
        
        # Calcular tendÃªncia simples
        if len(temps) >= 10:
            recent_avg = sum(temps[-10:]) / 10
            older_avg = sum(temps[:10]) / 10
            trend_diff = recent_avg - older_avg
            
            if trend_diff > 1:
                trend = "ğŸ“ˆ Crescente (+" + f"{trend_diff:.1f}Â°C)"
            elif trend_diff < -1:
                trend = "ğŸ“‰ Decrescente (" + f"{trend_diff:.1f}Â°C)"
            else:
                trend = "â¡ï¸ EstÃ¡vel"
        else:
            trend = "Dados insuficientes"
        
        analysis = f"""
**AnÃ¡lise AutomÃ¡tica** (sem IA)

ğŸŒ¡ï¸ **Faixa de Temperatura**
   MÃ­nima: {min(temps):.1f}Â°C | MÃ¡xima: {max(temps):.1f}Â°C | MÃ©dia: {sum(temps)/len(temps):.1f}Â°C

ğŸ“Š **TendÃªncia Recente**
   {trend}

âš ï¸ **Anomalias**
   {anomalies} variaÃ§Ãµes bruscas detectadas
   {'   âš ï¸ ATENÃ‡ÃƒO: Muitas variaÃ§Ãµes!' if anomalies > len(temps) * 0.1 else '   âœ… Comportamento normal'}

ğŸ’¡ **RecomendaÃ§Ã£o**
   {'   Investigar causa das variaÃ§Ãµes bruscas' if anomalies > 5 else '   Sistema operando dentro dos parÃ¢metros esperados'}
"""
        
        if statistics:
            stdev = statistics.get('stdev', 0)
            if stdev > 5:
                analysis += f"\n   âš ï¸ Alta variabilidade (Ïƒ={stdev:.1f}Â°C)"
        
        return {
            'ai_powered': False,
            'analysis': analysis.strip(),
            'timestamp': datetime.now().isoformat(),
            'data_points': len(readings),
            'trend': trend,
            'anomalies': anomalies
        }
    
    def generate_report(self, readings, statistics, analysis):
        """Gera relatÃ³rio textual completo"""
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        RELATÃ“RIO DE ANÃLISE DE TEMPERATURA                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Data/Hora: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
PerÃ­odo analisado: {statistics.get('period_hours', 24)}h
Pontos de dados: {statistics.get('count', 0)}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ESTATÃSTICAS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Temperatura MÃ­nima:     {statistics.get('min', 0):.2f}Â°C
Temperatura MÃ¡xima:     {statistics.get('max', 0):.2f}Â°C
Temperatura MÃ©dia:      {statistics.get('avg', 0):.2f}Â°C
Desvio PadrÃ£o:          {statistics.get('stdev', 0):.2f}Â°C
Anomalias Detectadas:   {statistics.get('anomalies', 0)}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ANÃLISE {'(IA - Google Gemini)' if analysis.get('ai_powered') else '(AutomÃ¡tica)'}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{analysis.get('analysis', 'Sem anÃ¡lise disponÃ­vel')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        return report
